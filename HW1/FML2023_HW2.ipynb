{"cells":[{"cell_type":"markdown","metadata":{"id":"bKXNjhiaxxUi"},"source":["# Fundamentals of Machine Learning (CSCI-UA.473)\n","\n","## Homework 2\n","### Due: October 26th, 2023 at 11:59PM\n","\n","### Name: Gil Halevi\n","### Email: gh2354@nyu.edu\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"AVL473j7W0CB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698636415196,"user_tz":240,"elapsed":18598,"user":{"displayName":"Gil Halevi","userId":"01950425314708180680"}},"outputId":"1740b17c-e7e9-45a3-fee2-94dc2eb93ac8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","## Use the same dataset that was released with HW1\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Load the entire dataset from the CSV file\n","data = pd.read_csv('/content/drive/MyDrive/FML/HW1/FML2023_HW1_Dataset.csv')\n","# data = pd.read_csv('FML2023_HW1_Dataset.csv')\n","# Separate the features, target values, and feature names\n","X = data.drop('target', axis=1)\n","y = data['target'].values\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"oD2ESfcW7ai7"},"source":["### Question 1: Maximum Likelihood Estimation (MLE) vs Maximum A Posteriori (MAP) (25 points)\n","\n","In Homework 1, we performed linear and ridge regression. To summarize:\n","\n","In Linear regression,\n","\n","$$\\beta = \\arg\\min_{\\beta}\\sum\\left(y_i - \\left(\\beta_0 + \\beta_1 x_{1i} +, \\ldots, + \\beta_px_{p i}\\right)\\right)^2$$\n","\n","\n","* $J(\\beta)$ is the cost function.\n","* $\\beta_0,\\ldots,\\beta_p$ are the coefficients for the features.\n","* $x_{1i}$ represents the values of the feature for the i-th observation.\n","* $y_i$ is the target value for the i-th observation.\n","\n","For ridge regression\n","\n","$$J(\\beta) = \\sum\\left(y_i - \\left(\\beta_0 + \\beta_1 x_{1i} +, \\ldots, + \\beta_px_{p i}\\right)\\right)^2 + \\lambda \\cdot \\sum \\beta_i^2$$\n","\n","* $\\lambda$ is the regularization hyper-parameter.\n","\n","**Task 1.1 (5 points)** Linear regression embodies Maximum Likelihood Estimation (MLE). Show that a closed form expression is $$\\beta = (\\mathbf{A}^\\top \\mathbf{A})^{-1}\\mathbf{A}^\\top \\mathbf{Y}$$ where $\\mathbf{A} = [X_1,\\ldots,X_n]$ and $\\mathbf{Y} = [Y_1,\\ldots,Y_n]$.\n","\n","Answer:\n","the initial expression can be written as $||Y-A\\beta||^2 = (Y-A\\beta)^T(Y-A\\beta) = Y^TY-Y^TA\\beta - \\beta^T A^T Y + \\beta^T A^T A \\beta$,\n","\n","where $\\beta = \\beta_1...\\beta_p$\n","\n","The gradient of this expression with respect to $\\beta$ is\n","$$-2A^TY + 2 A^T A \\beta$$\n","Setting this to 0, we get\n","\n","$$A^T A \\beta = A^T Y$$\n","$$\\beta = (A^T A)^{-1}A^T Y$$\n","**Task 1.2 (5 points)**: Ridge regression embodies Maximum A Posteriori (MAP), wherein the regularizer serves as the prior. Show that a closed form expression for the ridge estimator is $$\\beta = (\\mathbf{A}^\\top \\mathbf{A} + \\lambda I)^{-1}\\mathbf{A}^\\top \\mathbf{Y}$$ where $\\mathbf{A} = [X_1,\\ldots,X_n]$ and $\\mathbf{Y} = [Y_1,\\ldots,Y_n]$.\n","\n","Similar to above, we have to minimize the expression\n","$Y^TY-Y^TA\\beta - \\beta^T A^T Y + \\beta^T A^T A \\beta -\\lambda \\beta$\n","\n","taking the gradient and setting it to 0, we get\n","$$-2A^TY + 2 A^T A \\beta - \\lambda \\beta = 0$$\n","$$(A^T A -\\lambda I) \\beta = A^T Y$$\n","$$\\beta = (A^T A - \\lambda I)^{-1}A^T Y$$\n","\n","**Task 1.3 Implementation (10 points):** Fill in the code below to differentiate between MLE and MAP.\n","\n","**Task 1.4 (5 points):**\n","* Do MLE and MAP yield distinct solutions as the sample size tends to infinity? Explain your answer.\n","\n","* Will the impact of prior be greater with a small or large sample size, and what is the underlying rationale for this phenomenon?\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"oleMgs0V7Zsl","executionInfo":{"status":"ok","timestamp":1698636415360,"user_tz":240,"elapsed":170,"user":{"displayName":"Gil Halevi","userId":"01950425314708180680"}},"outputId":"98ee815a-1826-4cb8-d0a3-662ec00d220b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["MSE using MLE: 2900.1936284934754\n"]}],"source":["def mle_linear_regression(X, y):\n","    X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n","    # Compute the MLE estimates using closed-form solution (HINT: Use np.linalg.inv)\n","    return np.linalg.inv(X.T @ X) @ X.T @ y\n","\n","# Calculate MLE estimates without bias\n","theta_mle = mle_linear_regression(X_train.to_numpy(), y_train)\n","\n","# Make predictions on the test set\n","X_test_with_ones = np.concatenate((X_test.to_numpy(), np.ones((X_test.shape[0],1))), axis=1)\n","y_preds = np.matmul(X_test_with_ones,theta_mle)\n","# Calculate Mean Squared Error (MSE)\n","mse_mle = np.mean((y_test - y_preds)**2)\n","print(f\"MSE using MLE: {mse_mle}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"cqvfD-8uF-Jl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698636415772,"user_tz":240,"elapsed":13,"user":{"displayName":"Gil Halevi","userId":"01950425314708180680"}},"outputId":"c1f1d534-d247-4b26-e04a-54ce6928d0ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["MSE using MAP: 2856.810366181945\n"]}],"source":["def map_linear_regression(X, y, lambda_reg):\n","    # Compute MAP estimates with L2 regularization\n","    X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n","    return np.linalg.inv((X.T @ X) + (lambda_reg*np.identity(X.shape[1]))) @ X.T @ y\n","\n","# Set the regularization parameter (lambda)\n","lambda_reg = 0.1\n","theta_map = map_linear_regression(X_train.to_numpy(), y_train, lambda_reg)\n","\n","# Make predictions on the test set\n","X_test_with_ones = np.concatenate((X_test.to_numpy(), np.ones((X_test.shape[0],1))), axis=1)\n","y_preds = np.matmul(X_test_with_ones,theta_map)\n","\n","# Calculate Mean Squared Error (MSE)\n","mse_map = np.mean((y_test - y_preds)**2)\n","print(f\"MSE using MAP: {mse_map}\")"]},{"cell_type":"markdown","source":["MLE and MAP yield the same solution as the sample size tends to infinity because the prediction loss will eclipse the regularization loss, since the prediction loss increases linearly with sample size while the regularization stays constant. In MAP terms, this means that the prior becomes less and less important as you see more data, and as you tend toward infinity it becomes irrelevant.\n","\n","The impact of the prior is greater with smaller sample size. As said above, when you have seen less data, you rely more on priors. This is because there are many different true parameters that could be consistent with a small sample, so you rely on your prior to determine which of these consistent true parameters is most likely."],"metadata":{"id":"iF0Uvr9wMjOg"}},{"cell_type":"markdown","metadata":{"id":"RwH8gND4XGvE"},"source":["### Question 2: Classification with imbalanced dataset (20 points)\n","\n","We are creating an imbalanced version of the target variable for the Z dataset. An imbalanced dataset means that one class is much more frequent than the other class. In our case, we will consider the two classes as follows:\n","\n","- Class 0: Z progression values that are below the 75th percentile of the original target variable.\n","- Class 1: Z progression values that are above the 75th percentile of the original target variable.\n","\n","By doing this, we are creating an imbalance where Class 0 will be more prevalent than Class 1, mimicking a common scenario in real-world imbalanced datasets."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"dQMA6OPsXD49","executionInfo":{"status":"ok","timestamp":1698636419502,"user_tz":240,"elapsed":320,"user":{"displayName":"Gil Halevi","userId":"01950425314708180680"}}},"outputs":[],"source":["from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","\n","# Shuffle the data\n","X, y = shuffle(X, y, random_state=42)\n","\n","# Create an imbalanced target variable\n","y_imbalanced = np.where(y > np.percentile(y, 75), 1, 0)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y_imbalanced, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"FJiy4EyUYX93"},"source":["\n","**Task 2.1 (3 points):**\n","- Create a SVM classifier with a linear kernel, then calculate accuracy, precision, recall, and F1 score using available library functions."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"eKLZyXkwSDw0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698636476268,"user_tz":240,"elapsed":146,"user":{"displayName":"Gil Halevi","userId":"01950425314708180680"}},"outputId":"124a856e-322a-4c38-95e0-9dfb151feb79"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'accuracy': 0.8876404494382022, 'precision': 0.7, 'recall': 0.5, 'f1': 0.5833333333333334}\n"]}],"source":["from sklearn import svm\n","import sklearn\n","### Add code here\n","svm_model = svm.LinearSVC()\n","svm_model.fit(X_train, y_train)\n","y_pred = svm_model.predict(X_test)\n","def model_scores(y_test, y_pred):\n","  accuracy = sklearn.metrics.accuracy_score(y_test,y_pred)\n","  precision = sklearn.metrics.precision_score(y_test,y_pred, zero_division=0)\n","  recall = sklearn.metrics.recall_score(y_test,y_pred,zero_division=0)\n","  f1 = sklearn.metrics.f1_score(y_test,y_pred)\n","  return {\"accuracy\":accuracy,\"precision\":precision, \"recall\":recall, \"f1\":f1}\n","print(model_scores(y_test,y_pred))"]},{"cell_type":"markdown","metadata":{"id":"i6mJ3w9TbWHT"},"source":["\n","**Task 2.2 (5 points):** What causes the metrics to exhibit lower values for the imbalanced dataset compared to those in homework 1?"]},{"cell_type":"markdown","source":["Since the model is trained on many more negative samples than positive one, it learns to optimize by classifying everything as negataive, as this results in pretty good performance. When the dataset is more balanced, classifying everything as positive or everything as negative results in much worse performance, so the model must instead learn"],"metadata":{"id":"guvlTQqMq4h1"}},{"cell_type":"markdown","metadata":{"id":"QPYn44O40aKX"},"source":["**Random oversampling** is one of the many techniques used to address the class imbalance problem. It involves increasing the number of instances in the minority class by randomly duplicating existing instances. This helps to balance the class distribution and can lead to improved performance for certain models.\n","\n","**Task 2.3 (2 points):** Calculate and display the following statistics for the target variable (y) before applying random oversampling:\n","  - Mean\n","  - Standard Deviation\n","  - Minimum\n","  - Maximum\n","\n","**Task 2.4 (5 points):** Perform random oversampling on the training set. After oversampling, calculate and display the same statistics for the oversampled target variable."]},{"cell_type":"code","source":["import numpy as np\n","print(y_imbalanced.shape[0])\n","def print_summary_stats(data):\n","  mean = np.mean(data)\n","  std = np.std(data)\n","  min = np.min(data)\n","  max = np.max(data)\n","  print(\"mean:\"+str(mean))\n","  print(\"stdev:\"+str(std))\n","  print(\"min:\"+str(min))\n","  print(\"max:\"+str(max))\n","print_summary_stats(y_imbalanced)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_WV8YAIkbSgS","executionInfo":{"status":"ok","timestamp":1698636479600,"user_tz":240,"elapsed":192,"user":{"displayName":"Gil Halevi","userId":"01950425314708180680"}},"outputId":"6327ddf2-faf2-4544-cda5-c6e274786eaa"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["442\n","mean:0.251131221719457\n","stdev:0.4336638458496972\n","min:0\n","max:1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PItrNXNsX7ou"},"outputs":[],"source":["# Apply Random Oversampling\n","positive_indices = np.where(y_imbalanced == 1)[0]\n","oversamples = np.random.choice(positive_indices,220)\n","y_balanced = np.concatenate((y_imbalanced,y_imbalanced[oversamples]))\n","X_balanced = pd.concat((X,X.iloc[oversamples]))\n","\n","\n","X_balanced_train, X_balanced_test, y_balanced_train, y_balanced_test = train_test_split(X, y_imbalanced, test_size=0.2, random_state=42)\n","print_summary_stats(y_balanced)"]},{"cell_type":"markdown","metadata":{"id":"Oel3kCeV2tOM"},"source":["**Task 2.5 (5 points):**\n","- Create another instance of SVM classifier with linear kernel, fit it on the oversampled data and calculate all the prior metrics for the oversampled model.\n","- Show the metrics with different regularization parameters {0.1, 1, 10, 100} on the linear kernel.\n","- Show the metrics with polynomial degrees {-1, 0, 3, 4} and observe how the model's complexity changes.\n","- Introduce different values for the regularization parameter in the RBF kernel and show how it balances the trade-off between maximizing the margin and minimizing classification error."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"LeictOez24d9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698636483583,"user_tz":240,"elapsed":500,"user":{"displayName":"Gil Halevi","userId":"01950425314708180680"}},"outputId":"c03d089e-f675-4094-9c15-8e7831c06cc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["linear model with reg 0.1 scores: {'accuracy': 0.8426966292134831, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n","linear model with reg 1 scores: {'accuracy': 0.8876404494382022, 'precision': 0.5, 'recall': 0.7, 'f1': 0.5833333333333334}\n","linear model with reg 10 scores: {'accuracy': 0.8876404494382022, 'precision': 0.5714285714285714, 'recall': 0.6666666666666666, 'f1': 0.6153846153846153}\n","linear model with reg 100 scores: {'accuracy': 0.8876404494382022, 'precision': 0.5714285714285714, 'recall': 0.6666666666666666, 'f1': 0.6153846153846153}\n","\n","poly model with degree 1 scores: {'accuracy': 0.8876404494382022, 'precision': 0.5714285714285714, 'recall': 0.6666666666666666, 'f1': 0.6153846153846153}\n","poly model with degree 0 scores: {'accuracy': 0.8426966292134831, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n","poly model with degree 3 scores: {'accuracy': 0.8651685393258427, 'precision': 0.42857142857142855, 'recall': 0.6, 'f1': 0.5}\n","poly model with degree 4 scores: {'accuracy': 0.8314606741573034, 'precision': 0.14285714285714285, 'recall': 0.4, 'f1': 0.21052631578947364}\n","\n","rbf model with reg 0.1 scores: {'accuracy': 0.8651685393258427, 'precision': 0.14285714285714285, 'recall': 1.0, 'f1': 0.25}\n","rbf model with reg 1 scores: {'accuracy': 0.8876404494382022, 'precision': 0.5714285714285714, 'recall': 0.6666666666666666, 'f1': 0.6153846153846153}\n","rbf model with reg 10 scores: {'accuracy': 0.8089887640449438, 'precision': 0.35714285714285715, 'recall': 0.38461538461538464, 'f1': 0.3703703703703704}\n","rbf model with reg 100 scores: {'accuracy': 0.7303370786516854, 'precision': 0.35714285714285715, 'recall': 0.25, 'f1': 0.2941176470588235}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]}],"source":["### Add code here\n","\n","def test_svm(model,X_test, y_test):\n","  y_pred = svm_model.predict(X_test)\n","\n","from sklearn import svm\n","linear_models=dict()\n","for reg in [0.1,1,10,100]:\n","  linear_models[reg]=svm.LinearSVC(C=reg)\n","  linear_models[reg].fit(X_balanced_train,y_balanced_train)\n","  y_pred = linear_models[reg].predict(X_balanced_test)\n","  print(\"linear model with reg {} scores: {}\".format(reg,model_scores(y_pred, y_balanced_test)))\n","poly_models=dict()\n","print()\n","for deg in [1,0,3,4]:\n","  poly_models[reg]=svm.SVC(kernel='poly',degree=deg)\n","  poly_models[reg].fit(X_balanced_train,y_balanced_train)\n","  y_pred = poly_models[reg].predict(X_balanced_test)\n","  print(\"poly model with degree {} scores: {}\".format(deg,model_scores(y_pred, y_balanced_test)))\n","rbf_models=dict()\n","print()\n","for reg in [0.1,1,10,100]:\n","  rbf_models[reg]=svm.SVC(kernel='rbf',C=reg)\n","  rbf_models[reg].fit(X_balanced_train,y_balanced_train)\n","  y_pred = rbf_models[reg].predict(X_balanced_test)\n","  print(\"rbf model with reg {} scores: {}\".format(reg,model_scores(y_pred, y_balanced_test)))\n"]},{"cell_type":"markdown","metadata":{"id":"oquatEvqroqa"},"source":["### Question 3: Naive Bayes Model (10 points)\n","\n","Implement the Naieve Bayes classifer on the Z dataset.\n","\n","We will assume that each continuous feature $X_i$ of $X$ follow a Gaussian distribution within each class $Y$.\n","\n","- For each class $c$, calculate the mean $(\\mu_c)$ and standard deviation $(\\sigma_c)$ for each feature. These parameters represent the central tendency and spread of the feature values within each class. They can be computed as:\n","\n","   \\begin{align*}\n","   \\mu_c^j &= \\frac{1}{N_c} \\sum_{i=1}^{N_c} X_i^j \\quad \\text{(mean of feature \\(j\\) in class \\(c\\))} \\\\\n","   \\sigma_c^j &= \\sqrt{\\frac{1}{N_c} \\sum_{i=1}^{N_c} (X_i^j - \\mu_c^j)^2} + \\epsilon \\quad \\text{(standard deviation of feature \\(j\\) in class \\(c\\))}\n","   \\end{align*}\n","     \n","   where $N_c$ is the number of data points in class $c$, and $\\varepsilon=1e^{-6}$ is a small constant added for numerical stability.\n","\n","- To make a prediction for a new data point $x$, calculate the probability of $x$ belonging to each class $c$ using the Gaussian probability density function:\n","\n","   \\begin{align*}\n","   P(X^j = x^j | Y = c) = \\frac{1}{\\sqrt{2\\pi}\\sigma_c^j} e^{-\\frac{1}{2}\\left(\\frac{x^j - \\mu_c^j}{\\sigma_c^j}\\right)^2}\n","   \\end{align*}\n","\n","- Calculate the class probability $P(Y = c | X = x)$ as the product of the probabilities of each feature:\n","\n","    \\begin{align*}\n","     P(Y = c | X = x) = P(Y = c) \\prod_{j=1}^{D} P(X^j = x^j | Y = c)\n","    \\end{align*}\n","\n","   where $D$ is the number of features.\n","\n","- Assign the class label to the class with the highest probability:\n","\n","    \\begin{align*}\n","     \\hat{Y} = \\arg\\max_{c} P(Y = c | X = x)\n","     \\end{align*}\n","\n","**Hint:** In the code for Gaussian Naive Bayes, we take logarithms in certain calculations. This is a common technique used to avoid numerical underflow, especially when working with small probabilities."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"JEIVwXWKrscC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698636490662,"user_tz":240,"elapsed":229,"user":{"displayName":"Gil Halevi","userId":"01950425314708180680"}},"outputId":"a79cf4c5-2370-48f4-feb8-8cd7903cdc9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8202247191011236\n"]}],"source":["import numpy as np\n","\n","\n","class GaussianNaiveBayes:\n","\n","    def fit(self, X, y):\n","        self.classes = np.unique(y)\n","        self.parameters = dict()\n","        if isinstance(X, pd.DataFrame):\n","          X = X.to_numpy()\n","        for c in self.classes:\n","          class_Xs = X[np.where(y==c)[0]]\n","          self.parameters[c] = {\"prior\":(class_Xs.shape[0])/X.shape[0],\"mean\":np.mean(class_Xs,axis=0),\"std\":np.std(class_Xs,axis=0)+1e-6 }\n","          # print(self.parameters[c])\n","\n","    def _calculate_log_likelihood(self, x, mean, std):\n","        ### Compute the log likelihood of the given data point with the means and stds\n","        log_prob = 0\n","        for i in range(x.shape[0]):\n","          log_prob += (1/2*(((x[i]-mean[i])/std[i])**2)) * (np.log(std[i] * np.sqrt(2*np.pi)))\n","        return log_prob\n","\n","    def _calculate_class_probability(self, x, c):\n","        c=c.item()\n","        p_y_c = self.parameters[c]['prior']\n","        sum_log_prob = 0\n","        for i in range(x.shape[0]):\n","          sum_log_prob+=self._calculate_log_likelihood(x,self.parameters[c]['mean'], self.parameters[c]['std'])\n","        return p_y_c * np.exp(sum_log_prob)\n","\n","    def predict(self, X):\n","      if isinstance(X, pd.DataFrame):\n","        X = X.to_numpy()\n","      predicted_classes=[]\n","      for i in range(X.shape[0]):\n","        class_ps = np.asarray([self._calculate_class_probability(X[i],c) for c in np.nditer(self.classes)])\n","        predicted_class = self.classes[np.argmax(class_ps)]\n","        predicted_classes.append(predicted_class)\n","      return np.asarray(predicted_classes)\n","\n","    def score(self, X, y):\n","        if isinstance(X, pd.DataFrame):\n","          X = X.to_numpy()\n","        y_pred = self.predict(X)\n","        accuracy = np.mean(y_pred == y)\n","        return accuracy\n","\n","# Initialize and train the Gaussian Naive Bayes classifier\n","gnb = GaussianNaiveBayes()\n","gnb.fit(X_train, y_train)\n","y_pred = gnb.predict(X_test)\n","accuracy = gnb.score(X_test, y_test)\n","print(f\"Accuracy: {accuracy}\")"]},{"cell_type":"markdown","metadata":{"id":"4czskeTTCMam"},"source":["### Question 4: ROC curve and AUROC (15 points)\n","\n","**Task 4.1 (3 points):** Imagine you are a public health researcher investigating the performance of a new diagnostic test for disease Z, which is a potentially life-threatening condition. The test is designed to identify individuals who have the disease. You have collected data from a group of 500 patients who were tested for disease Z, and the results are as follows:\n","\n","Out of 150 patients who actually have disease Z, the test correctly identified 120 of them as positive.\n","However, the test also falsely identified 50 patients who do not have disease Z as positive.\n","\n","* **Precision:** Define precision in the context of this diagnostic test for disease Z. Calculate the precision of the test based on the provided data.\n","* **Recall:** Explain what recall means in this scenario. Calculate the recall of the test based on the provided data.\n","* **F1-score:** Define the F1-score and explain why it is important, especially in the context of diagnosing a serious disease like Z. Calculate the F1-score of the test based on the provided data.\n","* **Specificity:** What is specificity, and why is it relevant when evaluating a diagnostic test like this one? Calculate the specificity of the test based on the provided data.\n","* **Balanced Accuracy:** Describe what balanced accuracy is and why it might be a useful metric in this situation. Calculate the balanced accuracy of the test based on the provided data."]},{"cell_type":"markdown","source":["Precision is the proportion of people diagnosed with disease Z that actually have disease Z. The precision in this case is 120/(120+50)=0.71\n","\n","Recall is the proportion of people with disease Z who were diagnosed with disease Z. The recall in this case is 120/150 = 0.8\n","\n","F1 score is a combined metric of recall and precision, the \"harmonic mean\" of the two: 2/(1/precision + 1/recall). This is important because for a serious disease, both false negatives and false positives are quite harmful and you want a model that has low rates of both. In this case the F1 score is 2/((1/0.8)+(1/0.71))=0.75\n","\n","Specificity is the proportion of people who do not have disease Z that are classified as not having disease Z. it is useful because you don't want to have many false positives, since treatment can be expensive and risky. In this case it is 300/350 = 0.86\n","\n","\n","Balanced accuracy is purely the average between the specificity and recall (sensitivity). It might be useful for the same reason as f1, when you want a combined metric to minimize both false positives and false negatives. Unlike F1 score, balanced accuracy also takes into account true positives. In this case it is (0.86+0.8)/2 = 0.83\n"],"metadata":{"id":"2rCXS7cOpCp8"}},{"cell_type":"markdown","metadata":{"id":"aPOZo4QM5V_y"},"source":["**Task 4.2 (6 Points)** Plot the ROC curve\n","\n","An ROC curve plots TPR (y-axis) vs. FPR (x-axis) at all classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives.\n","\n","See this for more details (https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n","\n","Plot the ROC curve for Disease Z HW1 dataset with SVM classifier. **Note that you are not allowed to use any library function to compute the ROC. You have to do it from scratch.**"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"aZnZs_QC5oxQ","colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"status":"ok","timestamp":1698636504290,"user_tz":240,"elapsed":456,"user":{"displayName":"Gil Halevi","userId":"01950425314708180680"}},"outputId":"efaa644a-e0df-4722-dc38-849dbad4ff12"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["## Your code to compute and plot ROC goes here\n","import matplotlib.pyplot as plt\n","\n","from sklearn import svm\n","import sklearn\n","\n","\n","fprs=[]\n","tprs=[]\n","l_model=svm.LinearSVC()\n","l_model.fit(X_balanced_train,y_balanced_train)\n","y_pred_raw = l_model.decision_function(X_balanced_test)\n","for threshold in np.arange(1.25,-1.3,-0.05):\n","  y_pred = y_pred_raw >= threshold\n","  tp = np.count_nonzero(np.logical_and(y_balanced_test == 1, y_pred == 1))\n","  fp = np.count_nonzero(np.logical_and(y_balanced_test == 0, y_pred == 1))\n","  tn = np.count_nonzero(np.logical_and(y_balanced_test == 0, y_pred == 0))\n","  fn = np.count_nonzero(np.logical_and(y_balanced_test == 1, y_pred == 0))\n","  tprs.append(tp / (tp + fn))\n","  fprs.append(fp / (tn + fp))\n","\n","plt.scatter(fprs,tprs)\n","plt.ylabel(\"tpr\")\n","plt.xlabel(\"fpr\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"G9sRjUMk592y"},"source":["**Task 4.3 (6 Points):** Compute the AUC of ROC\n","\n","AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.\n","\n","Compute the AUC of your SVM model. **Note that you are not allowed to use any library function to compute the AUC. You have to do it from scratch.**"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"DOBw8ACA587S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698636513814,"user_tz":240,"elapsed":133,"user":{"displayName":"Gil Halevi","userId":"01950425314708180680"}},"outputId":"a754f61e-b4e6-431d-98f8-d13477c35ef8"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.797142857142857\n"]}],"source":["## Your code to compute the AUC goes here\n","auc = 0\n","for i in range(len(fprs)-1):\n","  auc += (tprs[i]+tprs[i+1])/2 * (fprs[i+1] - fprs[i])\n","print(auc)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}